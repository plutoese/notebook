{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 深度学习入门"
      ],
      "metadata": {
        "Collapsed": "false",
        "toc-hr-collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 感知机（perceptron）"
      ],
      "metadata": {
        "Collapsed": "false",
        "toc-hr-collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 基础知识\n",
        "\n",
        "感知机是神经网络（深度学习）起源的算法。\n",
        "\n",
        "感知机接收多个输入信号，输出一个信号。\n",
        "\n",
        "<br>\n",
        "\n",
        "下图为有两个输入的感知机\n",
        "\n",
        "<img src=\"https://p193.p3.n0.cdn.getcloudapp.com/items/ApuGEAz8/dp01.png?v=8324025e8b2ae9c6c0c341bfe7f15d5c\" width = 30% height = 30% />\n",
        "\n",
        "$x_{1}, x_{2}$是输入信号，$y$是输出信号，$w_{1}, w_{2}$是权重。图中的○称为“神经元”或者“节点”。输入信号被送往神经元时，会被分别乘以固定的权重（$w_{1} x_{1}, w_{2} x_{2}$）。神经元会计算传送过来的信号的总和，只有当这个总和超过了某个界限值时，才会输出$1$。这也称为“神经元被激活” 。这里将这个界限值称为阈值，用符号$\\theta$表示。\n",
        "\n",
        "把上述内容用数学式来表示\n",
        "\n",
        "$$\n",
        "y=\\left\\{\\begin{array}{ll}{0} & {\\left(w_{1} x_{1}+w_{2} x_{2} \\leqslant \\theta\\right)} \\\\ {1} & {\\left(w_{1} x_{1}+w_{2} x_{2}>\\theta\\right)}\\end{array}\\right.\n",
        "$$\n",
        "\n",
        "感知机的多个输入信号都有各自固有的权重，这些权重发挥着控制各个信号的重要性的作用。也就是说，权重越大，对应该权重的信号的重要性就越高。"
      ],
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 用感知机实现逻辑电路"
      ],
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 实现与门\n",
        "\n",
        "<img src=\"./pic/dp02.png\" width = 36% height = 36% />\n",
        "\n",
        "实际上，满足上述条件的参数的选择方法有无数多个。比如，当$\\left(w_{1}, w_{2}, \\theta\\right)=(0.5,0.5,0.7)$时。此外，当$\\left(w_{1}, w_{2}, \\theta\\right)$为$(0.5,0.5,0.8)$或$(1.0,1.0,1.0)$者时，同样也满足与门的条件。"
      ],
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 与门的实现\n",
        "def AND(x1, x2):\n",
        "    w1, w2, theta = 0.5, 0.5, 0.7\n",
        "    tmp = x1*w1 + x2*w2\n",
        "    if tmp <= theta:\n",
        "        return 0\n",
        "    elif tmp > theta:\n",
        "        return 1\n",
        "\n",
        "print(AND(0, 0), AND(1, 0), AND(0, 1), AND(1, 1))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0 0 1\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 导入权重和偏执\n",
        "\n",
        "为了理解和计算方便，我们把之前感知机的形式略作调整，得到\n",
        "\n",
        "$$\n",
        "y=\\left\\{\\begin{array}{ll}{0} & {\\left(b+w_{1} x_{1}+w_{2} x_{2} \\leqslant 0\\right)} \\\\ {1} & {\\left(b+w_{1} x_{1}+w_{2} x_{2}>0\\right)}\\end{array}\\right.\n",
        "$$\n",
        "\n",
        "此处，$b$称为偏置，$w_{1}$和$w_{2}$称为权重。具体而言，$w_{1}$和$w_{2}$是控制输入信号的重要性的参数，而偏置是调整神经元被激活的容易程度（输出信号为1 的程度）的参数。"
      ],
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 与门的另一种实现\n",
        "def AND(x1, x2):\n",
        "    x = np.array([x1, x2])\n",
        "    w = np.array([0.5, 0.5])\n",
        "    b = -0.7\n",
        "    tmp = np.sum(w*x) + b\n",
        "    if tmp <= 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 实现与非门和或门"
      ],
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 与非门\n",
        "\n",
        "<img src=\"./pic/dp03.png\" width = 36% height = 36% />"
      ],
      "metadata": {
        "Collapsed": "false",
        "toc-hr-collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 与非门实现\n",
        "def NAND(x1, x2):\n",
        "    x = np.array([x1, x2])\n",
        "    w = np.array([-0.5, -0.5]) # 仅权重和偏置与AND不同！\n",
        "    b = 0.7\n",
        "    tmp = np.sum(w*x) + b\n",
        "    if tmp <= 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 或门\n",
        "\n",
        "<img src=\"./pic/dp04.png\" width = 36% height = 36% />"
      ],
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def OR(x1, x2):\n",
        "    x = np.array([x1, x2])\n",
        "    w = np.array([0.5, 0.5]) # 仅权重和偏置与AND不同！\n",
        "    b = -0.2\n",
        "    tmp = np.sum(w*x) + b\n",
        "    if tmp <= 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 感知机的局限性"
      ],
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 异或门\n",
        "\n",
        "<img src=\"./pic/dp05.png\" width = 30% height = 30% />\n",
        "\n",
        "感知机是无法实现这个异或门的。\n",
        "\n",
        "<img src=\"./pic/dp06.png\" width = 36% height = 36% />\n",
        "\n",
        "因为无法找到一条直线来分割空间。"
      ],
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 多层感知机\n",
        "\n",
        "使用与非门和或门作为中间层实现异或门。\n",
        "\n",
        "<img src=\"./pic/dp07.png\" width = 36% height = 36% />"
      ],
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 或非门的实现\n",
        "def XOR(x1, x2):\n",
        "    s1 = NAND(x1, x2)\n",
        "    s2 = OR(x1, x2)\n",
        "    y = AND(s1, s2)\n",
        "    return y\n",
        "\n",
        "XOR(0, 0) # 输出0\n",
        "XOR(1, 0) # 输出1\n",
        "XOR(0, 1) # 输出1\n",
        "XOR(1, 1) # 输出0"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "0"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "异或门是一种多层结构的神经网络。这里，将最左边的一列称为第0 层，中间的一列称为第1层，最右边的一列称为第2层。叠加了多层的感知机也称为多层感知机（multi-layered perceptron）。\n",
        "\n",
        "<img src=\"./pic/dp08.png\" width = 45% height = 45% />"
      ],
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 神经网络\n",
        "\n",
        "神经网络的一个重要性质是它可以自动地从数据中学习到合适的权重参数。"
      ],
      "metadata": {
        "Collapsed": "false",
        "toc-hr-collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 从感知机到神经网络\n",
        "\n",
        "用图来表示神经网络的话，我们把最左边的一列称为输入层，最右边的一列称为输出层，中间的一列称为中间层。中间层有时也称为隐藏层。\n",
        "\n",
        "<img src=\"./pic/dp09.png\" width = 45% height = 45% />\n",
        "\n",
        "回顾感知机\n",
        "\n",
        "<img src=\"./pic/dp11.png\" width = 30% height = 30% />\n",
        "\n",
        "引入新函数$h(x)$\n",
        "\n",
        "$$\n",
        "y=h\\left(b+w_{1} x_{1}+w_{2} x_{2}\\right)\n",
        "$$"
      ],
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 使用激活函数\n",
        "\n",
        "$h(x)$函数会将输入信号的总和转换为输出信号，这种函数一般称为激活函数（activation function）\n",
        "\n",
        "激活函数的计算过程可如图表示\n",
        "\n",
        "<img src=\"./pic/dp12.png\" width = 30% height = 30% />"
      ],
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 激活函数\n",
        "\n",
        "激活函数以阈值为界，一旦输入超过阈值，就切换输出。这样的函数称为“阶跃函数”。因此，可以说感知机中使用了阶跃函数作为激活函数。如果将激活函数从阶跃函数换成其他函数，就可以进入神经网络的世界了。"
      ],
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### sigmoid 函数\n",
        "\n",
        "神经网络中经常使用的一个激活函数就是sigmoid函数（sigmoid function）。\n",
        "\n",
        "$$\n",
        "h(x)=\\frac{1}{1+\\exp (-x)}\n",
        "$$"
      ],
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sigmoid函数的实现\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### sigmoid函数和阶跃函数的比较\n",
        "\n",
        "<img src=\"./pic/dp13.png\" width = 45% height = 45% />"
      ],
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### RELU函数\n",
        "\n",
        "ReLU函数在输入大于0 时，直接输出该值；在输入小于等于0 时，输出0。\n",
        "\n",
        "$$\n",
        "h(x)=\\left\\{\\begin{array}{ll}{x} & {(x>0)} \\\\ {0} & {(x \\leqslant 0)}\\end{array}\\right.\n",
        "$$\n",
        "\n",
        "<img src=\"./pic/dp14.png\" width = 45% height = 45% />"
      ],
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "    return np.maximum(0, x)"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 输出层的设计\n",
        "\n",
        "一般而言，回归问题用恒等函数，分类问题用softmax函数。\n",
        "\n",
        "分类问题中使用的softmax 函数可以用下面的式子表示\n",
        "\n",
        "$$\n",
        "y_{k}=\\frac{\\exp \\left(a_{k}\\right)}{\\sum_{i=1}^{n} \\exp \\left(a_{i}\\right)}\n",
        "$$"
      ],
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(a):\n",
        "    c = np.max(a)\n",
        "    exp_a = np.exp(a - c) # 溢出对策\n",
        "    sum_exp_a = np.sum(exp_a)\n",
        "    y = exp_a / sum_exp_a\n",
        "    \n",
        "    return y"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 神经网络的学习"
      ],
      "metadata": {
        "Collapsed": "false",
        "toc-hr-collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 从数据中学习\n",
        "\n",
        "神经网络的特征就是可以从数据中学习。所谓“从数据中学习”，是指可以由数据自动决定权重参数的值。\n",
        "\n",
        "<img src=\"./pic/dp15.png\" width = 60% height = 60% />"
      ],
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 损失函数\n",
        "\n",
        "神经网络以某个指标为线索寻找最优权重参数。神经网络的学习中所用的指标称为损失函数（loss function）。这个损失函数可以使用任意函数，但一般用均方误差和交叉熵误差等。"
      ],
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 均方误差\n",
        "\n",
        "可以用作损失函数的函数有很多，其中最有名的是均方误差（mean squared error）。均方误差如下式所示。\n",
        "\n",
        "$$\n",
        "E=\\frac{1}{2} \\sum_{k}\\left(y_{k}-t_{k}\\right)^{2}\n",
        "$$\n",
        "\n",
        "这里，$y_{k}$是表示神经网络的输出，$t_{k}$表示监督数据，$k$表示数据的维数。"
      ],
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 均方误差的实现\n",
        "def mean_squared_error(y, t):\n",
        "    return 0.5 * np.sum((y-t)**2)"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 交叉熵误差\n",
        "\n",
        "除了均方误差之外，交叉熵误差（cross entropy error）也经常被用作损失函数。交叉熵误差如下式所示。\n",
        "\n",
        "$$\n",
        "E=-\\sum_{k} t_{k} \\log y_{k}\n",
        "$$\n",
        "\n",
        "这里，$t_{k}$正确解标签。"
      ],
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 交叉熵误差的实现\n",
        "def cross_entropy_error(y, t):\n",
        "    delta = 1e-7\n",
        "    return -np.sum(t * np.log(y + delta))"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### mini-batch学习\n",
        "\n",
        "神经网络的学习也是从训练数据中选出一批数据（称为mini-batch, 小批量），然后对每个mini-batch 进行学习。比如，从60000个训练数据中随机选择100笔，再用这100笔数据进行学习。这种学习方式称为mini-batch学习。"
      ],
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 梯度\n",
        "\n",
        "像$\\left(\\frac{\\partial f}{\\partial x_{0}}, \\frac{\\partial f}{\\partial x_{1}}\\right)$这样的由全部变量的偏导数汇总而成的向量称为梯度（gradient）。\n",
        "\n",
        "<img src=\"./pic/dp16.png\" width = 55% height = 55% />"
      ],
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 梯度法\n",
        "\n",
        "机器学习的主要任务是在学习时寻找最优参数。同样地，神经网络也必须在学习时找到最优参数（权重和偏置）。这里所说的最优参数是指损失函数取最小值时的参数。但是，一般而言，损失函数很复杂，参数空间庞大，我们不知道它在何处能取得最小值。而通过巧妙地使用梯度来寻找函数最小值（或者尽可能小的值）的方法就是梯度法。梯度表示的是各点处的函数值减小最多的方向。\n",
        "\n",
        "在梯度法中，函数的取值从当前位置沿着梯度方向前进一定距离，然后在新的地方重新求梯度，再沿着新梯度方向前进，如此反复，不断地沿梯度方向前进。像这样，通过不断地沿梯度方向前进，逐渐减小函数值的过程就是梯度法（gradient method）。梯度法是解决机器学习中最优化问题的常用方法，特别是在神经网络的学习中经常被使用。\n",
        "\n",
        "**超参数**\n",
        "\n",
        "这是一种和神经网络的参数（权重和偏置）性质不同的参数。相对于神经网络的权重参数是通过训练数据和学习算法自动获得的，学习率这样的超参数则是人工设定的。一般来说，超参数需要尝试多个值，以便找到一种可以使学习顺利进行的设定。"
      ],
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 学习算法的实现\n",
        "\n",
        "神经网络的学习分成下面4个步骤。\n",
        "\n",
        "- 步骤1（mini-batch）\n",
        "\n",
        "从训练数据中随机选出一部分数据，这部分数据称为mini-batch。我们的目标是减小mini-batch 的损失函数的值。\n",
        "\n",
        "- 步骤2（计算梯度）\n",
        "\n",
        "为了减小mini-batch 的损失函数的值，需要求出各个权重参数的梯度。梯度表示损失函数的值减小最多的方向。\n",
        "\n",
        "- 步骤3（更新参数）\n",
        "\n",
        "将权重参数沿梯度方向进行微小更新。\n",
        "\n",
        "- 步骤4（重复）\n",
        "\n",
        "重复步骤1、步骤2、步骤3。"
      ],
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 误差反向传播法\n",
        "\n",
        "一个能够高效计算权重参数的梯度的方法。"
      ],
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 计算图\n",
        "\n",
        "计算图将计算过程用图形表示出来。"
      ],
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 用计算图求解\n",
        "\n",
        "问题：某人在超市买了2个100日元一个的苹果，消费税是10%，请计算支付金额。\n",
        "\n",
        "<img src=\"./pic/dp17.png\" width = 55% height = 55% />\n",
        "<img src=\"./pic/dp18.png\" width = 55% height = 55% />\n",
        "\n",
        "<br>\n",
        "\n",
        "问题：某人在超市买了2个苹果、3个橘子。其中，苹果每个100日元，橘子每个150日元。消费税是10%，请计算支付金额。\n",
        "\n",
        "<img src=\"./pic/dp19.png\" width = 55% height = 55% />\n",
        "\n",
        "<br>\n",
        "\n",
        "综上，用计算图解题的情况下，需要按如下流程进行。\n",
        "\n",
        "- 构建计算图。\n",
        "- 在计算图上，从左向右进行计算。\n",
        "\n",
        "这里的第2 歩“从左向右进行计算”是一种正方向上的传播，简称为正向传播（forward propagation）。正向传播是从计算图出发点到结束点的传播。"
      ],
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 链式法则"
      ],
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 计算图的反向传播\n",
        "\n",
        "假设存在$y=f(x)$的计算，这个计算的反向传播如图。\n",
        "\n",
        "<img src=\"./pic/dp20.png\" width = 55% height = 55% />"
      ],
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 反向传播"
      ],
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 加法节点的反向传播\n",
        "\n",
        "加法节点的反向传播只乘以1，所以输入的值会原封不动地流向下一个节点。、\n",
        "\n",
        "<img src=\"./pic/dp21.png\" width = 55% height = 55% />"
      ],
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 乘法节点的反向传播\n",
        "\n",
        "乘法的反向传播会将上游的值乘以正向传播时的输入信号的“翻转值”后传递给下游。\n",
        "\n",
        "<img src=\"./pic/dp22.png\" width = 55% height = 55% />"
      ],
      "metadata": {
        "Collapsed": "false"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 苹果的例子\n",
        "\n",
        "思考购买苹果的例子（2个苹果和消费税）。这里要解的问题是苹果的价格、苹果的个数、消费税这3个变量各自如何影响最终支付的金额。这个问题相当于求“支付金额关于苹果的价格的导数”“支付金额关于苹果的个数的导数”“支付金额关于消费税的导数”。\n",
        "\n",
        "<img src=\"./pic/dp23.png\" width = 55% height = 55% />"
      ],
      "metadata": {
        "Collapsed": "false"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "nteract": {
      "version": "0.27.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}