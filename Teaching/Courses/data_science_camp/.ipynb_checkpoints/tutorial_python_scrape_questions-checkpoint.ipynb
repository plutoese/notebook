{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据采集——Python爬虫简介习题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 理论习题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 请问网易(www.163.com) 的IP地址是下述哪一个？(D)\n",
    "\n",
    "- A. 115.238.190.240\n",
    "- B. 180.101.49.12\n",
    "- C. 101.227.95.3\n",
    "- D. 218.1.70.80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 超文本标记语言的缩写是下列哪一个？（C）\n",
    "\n",
    "- A. HTTP\n",
    "- B. FTP\n",
    "- C. HTML\n",
    "- D. CSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 如果通过Python库requests的get()方法成功获取一个网页，那么返回的HTTP状态码为（A）？\n",
    "\n",
    "- A. 200\n",
    "- B. 404\n",
    "- C. 500\n",
    "- D. 000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 若使用下述代码获得网页，请问下列哪个属性或方法不属于r对象？（C）\n",
    "\n",
    "```\n",
    "import requests\n",
    "r = requests.get(\"http://www.baidu.com/\")\n",
    "```\n",
    "\n",
    "- A. ok\n",
    "- B. status_code\n",
    "- C. string\n",
    "- D. text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 对于上海车牌，假设基本规则为车牌号格式：沪 + 1位大写字母 + 5位大写字母或者数字，请问下列哪个正则表达式可用于匹配上述格式的车牌？（D）\n",
    "\n",
    "- A. \"^沪[A-Z][0-9]{5}\\$\"\n",
    "- B. \"^沪[A-Z][A-Z][0-9]{4}\\$\"\n",
    "- C. \"^沪[0-9]{6}\\$\"\n",
    "- D. \"^沪[A-Z][A-Z0-9]{5}\\$\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 在HTML语言中，链接一般有下述哪个标签创建？（A）\n",
    "\n",
    "- A. \\<a\\>\n",
    "- B. \\<p\\>\n",
    "- C. \\<tr\\>\n",
    "- D. \\<title\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 下面哪个Python库不能用来抓取网页（D）\n",
    "\n",
    "- A. urilib\n",
    "- B. requests\n",
    "- C. aiohttp\n",
    "- D. bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. 下列哪种方法不能够返回网页中**所有的**\\<a\\>标签？（）\n",
    "\n",
    "- A. bs_obj.select(\"a\")\n",
    "- B. bs_obj.find(\"a\")\n",
    "- C. bs_obj.find_all(\"a\")\n",
    "- D. bs_obj(\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. 在bs4库中，若find_all()方法无法查找到相关的标签节点，那么它将返回（C）\n",
    "\n",
    "- A. None\n",
    "- B. -1\n",
    "- C. 空列表\n",
    "- D. False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. 若由于网页的内容是通过JS动态生成的，那么可以尝试用下列哪种方法获取数据？（D）\n",
    "\n",
    "- A. 使用requests库的post方法\n",
    "- B. 换一个IP地址\n",
    "- C. 设置更长时间的timeout\n",
    "- D. 通过XHR的方式获取实际数据的网址"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 操作习题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 爬取网页(http://www.fortunechina.com/fortune500/c/2019-07/22/content_339535.htm) 中的世界500强企业名单，并导出为excel文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference program\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_web_page_table(website, table_css_selector, encoding=\"UTF-8\", header=True):\n",
    "    # to crawl the web page\n",
    "    r = requests.get(website)\n",
    "\n",
    "    # to correct coding\n",
    "    r.encoding = encoding\n",
    "\n",
    "    # to create BS object for parse HTML\n",
    "    bs_obj = BeautifulSoup(r.text, 'lxml')\n",
    "    \n",
    "    # first step: find the table\n",
    "    table = bs_obj.select(table_css_selector)[0]\n",
    "    \n",
    "    row_data = []\n",
    "    for row in table.select(\"tr\"):\n",
    "        row_data.append([item.text for item in row.select(\"td\")])\n",
    "    \n",
    "    row_data = []\n",
    "    for row in table.select(\"tr\"):\n",
    "        # to add table header\n",
    "        if len(row.select(\"th\")) > 0:\n",
    "            row_data.append([re.sub(\"\\s\", \"\", item.text) for item in row.select(\"th\")])\n",
    "        else:\n",
    "            row_data.append([re.sub(\"\\s\", \"\", item.text) for item in row.select(\"td\")])\n",
    "            \n",
    "    if header:\n",
    "        return pd.DataFrame(row_data[1:], columns=row_data[0])\n",
    "    else:\n",
    "        return pd.DataFrame(row_data)\n",
    "\n",
    "result = get_web_page_table(\"http://www.fortunechina.com/fortune500/c/2019-07/22/content_339535.htm\", \"#table1\", encoding=\"UTF-8\")\n",
    "result.to_excel(\"result.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 通过高德交通大地图(https://report.amap.com/index.do )，抓取2020年第一季度某个城市的拥堵延时指数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference program\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "r = requests.get('https://report.amap.com/ajax/cityDailyQuarterly.do?cityCode=310000&year=2020&quarter=1')\n",
    "pdata = pd.DataFrame(r.json())\n",
    "pdata = pdata.rename(columns={\"categories\":\"date\", \"serieData\":\"value\"})\n",
    "pdata.to_excel(\"result.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
